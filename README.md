# ai-toolkit

> **Versi√≥n actual:** `v1.0`  
> **Estado:** estable (educacional / experimental)

**Herramientas de IA para backend (FastAPI)**

`ai-toolkit` es una **API educativa y experimental** construida en **FastAPI** para explorar **c√≥mo dise√±ar sistemas backend con LLMs de forma profesional**, poniendo foco en:

- control expl√≠cito del comportamiento del modelo
- validaci√≥n estricta del output
- manejo consciente de errores y retries
- arquitectura desacoplada y extensible
- observabilidad y medici√≥n de costos

> üéØ **Objetivo del proyecto**  
> No es un producto final, sino un **laboratorio backend** para demostrar **criterio arquitect√≥nico real** en sistemas con IA: c√≥mo se dise√±an, c√≥mo evolucionan y c√≥mo se preparan para un entorno empresarial.

Este README documenta **el alcance cerrado de la versi√≥n v1.0** y describe **la evoluci√≥n planificada hacia v2 y v3**.

---

## Estado actual ‚Äì v1.0 (RAG baseline)

La versi√≥n **v1.0** representa el **baseline funcional del proyecto**:
un sistema RAG completamente operativo, dise√±ado para priorizar **claridad, control y correcci√≥n** por sobre escalabilidad.

Cuenta con una **demo privada en la nube** para validaci√≥n funcional.

---

## Funcionalidades incluidas en v1.0

### RAG (n√∫cleo del sistema)

- Ingesta de documentaci√≥n v√≠a URL
- Limpieza y normalizaci√≥n por tipo de fuente
- Chunking **espec√≠fico por tipo de documento**:
  - HTML: separaci√≥n por `<h2>` / `<h3>`
  - README / Markdown: secciones sem√°nticas
  - PDF y texto plano: tama√±o fijo
- Strategy Pattern para chunking
- Embeddings locales con `sentence-transformers`
  - creaci√≥n por batches
  - manejo de errores (timeouts, respuestas vac√≠as, retry simple)
- Vector store abstra√≠do (implementaci√≥n actual: Qdrant)
- Batch insert de chunks
- Metadata por chunk (`source`, `domain`, `topic`, `chunk_index`)
- Query con embedding de consulta
- Filtros din√°micos por dominio y tem√°tica
- Re-ranking simple con Cross-Encoder
- Construcci√≥n expl√≠cita del contexto enviado al LLM
- Respuestas con citaciones por chunk
- Streaming de respuesta

---

### Observabilidad y control (v1.0)

- Logs estructurados
- Medici√≥n de tiempo de respuesta del LLM
- Tracking de tokens consumidos
- Estimaci√≥n de costo por request

---

### Frontend (demo funcional)

- Ingesta de URLs y PDFs
- Chat con streaming
- Citations visibles
- Estados de carga y error
- Inputs opcionales de dominio y tem√°tica
- Panel simple de estado

---

## Filosof√≠a de dise√±o

El proyecto prioriza deliberadamente:

- **Transparencia del flujo**  
  Cada paso del pipeline es expl√≠cito y trazable.
- **Separaci√≥n de responsabilidades**  
  API, l√≥gica de negocio y proveedores est√°n claramente desacoplados.
- **Control del riesgo**  
  Validaci√≥n, retries y errores se manejan de forma consciente.
- **Intercambiabilidad de componentes**  
  LLMs, embeddings y vector stores pueden reemplazarse sin afectar el core.

No se oculta complejidad: se **expone para poder aprenderla**.

---

## Arquitectura general (v1.0)

```ascii
HTTP (FastAPI)
‚Üì
Routers (API layer)
‚Üì
Services (orquestaci√≥n expl√≠cita)
‚Üì
Clients / Providers
‚îú‚îÄ LLM providers
‚îú‚îÄ Embedding providers
‚îî‚îÄ Vector store clients
```

---

## Limitaciones conocidas de v1.0

Esta versi√≥n **no est√° orientada a producci√≥n**.  
Por dise√±o:

- la ingesta se realiza de forma s√≠ncrona
- el estado se mantiene en memoria
- no hay workers ni colas de procesamiento

Estas decisiones fueron intencionales para:

- simplificar el flujo
- priorizar comprensi√≥n y control
- establecer un baseline claro

---

## Versionado conceptual del proyecto

El proyecto evoluciona por **versiones conceptuales**, cada una con objetivos claros.

---

## v1.0 ‚Äì RAG baseline (actual)

**Enfoque**

- RAG expl√≠cito y controlado
- Arquitectura limpia
- Observabilidad b√°sica
- Correctness del output

**No incluye**

- Procesamiento asincr√≥nico
- Workers o colas
- M√©tricas persistentes
- Modelos locales
- Agentes

---

## v2.0 ‚Äì RAG asincr√≥nico + observabilidad (en desarrollo)

La versi√≥n **v2.0** extiende v1 hacia un **escenario enterprise-like**, mostrando c√≥mo escalar el sistema sin perder control.

### Objetivos de v2

- Separar API y procesamiento pesado
- Introducir procesamiento asincr√≥nico
- Mejorar observabilidad t√©cnica
- Mantener el mismo pipeline RAG, pero ejecutado por workers

### Cambios principales

- Procesamiento asincr√≥nico con **Celery**
- Broker y backend de estado (Redis)
- Ingesta de documentos fuera del request HTTP
- Estado de tareas accesible por `job_id`
- M√©tricas del pipeline:
  - latencia
  - tokens
  - errores
- Factory de LLM providers
- Comparaci√≥n entre LLM remoto y modelo local (Ollama)

### Arquitectura v2 (alto nivel)

```ascii
Cliente / Frontend
‚Üì
FastAPI (API layer)

validaci√≥n

creaci√≥n de job_id

dispatch de tareas
‚Üì
Broker (Redis)
‚Üì
Celery Worker

extracci√≥n

limpieza

chunking

embeddings

inserci√≥n en vector store
```

---

## v3.0 ‚Äì MCP + Agent orchestration (exploratorio)

La versi√≥n **v3.0** explora patrones avanzados de sistemas con IA.

### Objetivo

- Exponer capacidades del sistema como **skills reutilizables**
- Introducir un **agente m√≠nimo**, no aut√≥nomo

El agente podr√° decidir:

- responder directamente
- usar RAG
- ejecutar una skill espec√≠fica

No se busca:

- autonom√≠a total
- loops largos
- sistemas auto-reflexivos

Esta versi√≥n es **experimental y educativa**.

---

## Instalaci√≥n local r√°pida

```bash
git clone https://github.com/SrStamm/ai-toolkit.git
cd ai-toolkit
docker-compose up --build
```
