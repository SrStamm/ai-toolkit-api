# ai-toolkit

> **Versi√≥n actual:** `v2.0`  
> **Estado:** estable (educacional / experimental)

**Herramientas de IA para backend (FastAPI)**

`ai-toolkit` es una API educativa y experimental construida en FastAPI para explorar c√≥mo dise√±ar sistemas backend con LLMs de forma profesional, poniendo foco en:

- control expl√≠cito del comportamiento del modelo
- validaci√≥n estricta del output
- manejo consciente de errores y retries
- arquitectura desacoplada y extensible
- observabilidad y m√©tricas

> üéØ Objetivo del proyecto
> No es un producto final, sino un laboratorio backend para demostrar criterio arquitect√≥nico real en sistemas con IA: c√≥mo se dise√±an, c√≥mo evolucionan y c√≥mo se preparan para un entorno enterprise-like.

---

## Estado actual ‚Äì v2.0 (RAG asincr√≥nico + observabilidad)

La versi√≥n v2.0 representa un salto hacia un sistema escalable, donde la ingesta y el procesamiento de datos se ejecutan fuera del request HTTP mediante Celery, manteniendo el pipeline RAG y a√±adiendo observabilidad avanzada.

### Objetivos de v2.0

- Separar API y procesamiento pesado
- Introducir procesamiento asincr√≥nico con workers
- Mejorar observabilidad t√©cnica
- Mantener el pipeline RAG, pero ejecutado por Celery
- Medir latencia, tokens, errores y uso de recursos
- Comparar LLM remoto vs LLM local (Ollama)

### Cambios principales

- FastAPI + Celery:
  - API solo recibe requests y crea job_id para tasks asincr√≥nicas
  - Workers realizan:
    - Extracci√≥n y limpieza de documentos
    - Chunking (HTML, Markdown, PDF)
    - Creaci√≥n de embeddings
    - Inserci√≥n en vector store (Qdrant)
    - Eliminaci√≥n de chunks antiguos o duplicados
- M√©tricas Prometheus:
  - Histogram de latencia por etapa: vector search, RAG pipeline, LLM
  - Tokens por request
  - Errores por etapa
  - Uso de fallback y circuit breaker
- Observabilidad:
  - Logs estructurados con structlog
  - Tracking de tokens consumidos y costo estimado
- LLM Factory:
  - Se puede cambiar entre modelo remoto (Mistral) o local (Ollama)
  - Dashboard en Grafana (pendiente en V2.1 para m√©tricas completas)

### Arquitectura v2.0

```ascii
Cliente / Frontend
‚Üì
FastAPI (API layer)
  - Validaci√≥n
  - Creaci√≥n de job_id
  - Dispatch de tareas a Celery
‚Üì
Broker / Backend (Redis)
‚Üì
Celery Worker
  - Extracci√≥n
  - Limpieza
  - Chunking
  - Embeddings
  - Inserci√≥n en Vector Store
  - Actualizaci√≥n de estado en Redis
‚Üì
Respuesta a Frontend v√≠a job_id
```

---

## Funcionalidades incluidas en v2.0

### Core RAG

- Ingesta de documentos v√≠a URL o archivos
- Chunking espec√≠fico por tipo de documento
- Strategy Pattern para chunking
- Embeddings locales y remotos con batching
- Re-ranking simple
- Construcci√≥n de contexto expl√≠cito para el LLM
- Streaming de respuesta
- Metadata por chunk (source, domain, topic, chunk_index)

### Observabilidad y m√©tricas

- Logs estructurados
- Decoradores de latencia por LLM y RAG
- M√©tricas Prometheus:
- Histogram de latencia por etapa
- Tokens consumidos
- Errores por etapa
- Fallbacks y circuit breaker
- M√©tricas espec√≠ficas para Celery:
- Duraci√≥n de tasks
- Status (success/error)
- Panel b√°sico de estado en Frontend

### Frontend

- Chat estilo RAG
- Inputs opcionales: dominio, topic
- Estado de carga y errores
- Citations por chunk
- Visualizaci√≥n parcial del progreso de tasks

---

## Roadmap de versiones siguientes

### V2.1 ‚Äì Observabilidad avanzada

- Histogram por etapa del RAG
- M√©trica de tama√±o promedio de chunks por request y size_tokens
- M√©trica de recall (top_k hit ratio, requiere dataset de prueba)
- Dashboard serio en Grafana

### V2.2 ‚Äì Performance profiling

- Benchmarks LLM local vs remoto
- Latencia de embedding vs tama√±o batch
- Throughput de Celery
- Optimizaci√≥n de batch insert y creaci√≥n de embeddings

### V3.0 ‚Äì MCP + Agent orchestration (exploratorio)

- Exponer capacidades como skills reutilizables
- Introducir un agente m√≠nimo que decida:
- Responder directamente
- Usar RAG
- Ejecutar una skill espec√≠fica
- Experimental: no loops largos ni autonom√≠a total

---

## Filosof√≠a de dise√±o

- Transparencia del flujo: cada paso del pipeline es trazable
- Separaci√≥n de responsabilidades: API, l√≥gica de negocio y proveedores desacoplados
- Control del riesgo: retries, errores y fallback expl√≠citos
- Intercambiabilidad de componentes: LLM, embeddings y vector stores reemplazables sin afectar el core

---

## Instalaci√≥n local r√°pida

```bash
git clone https://github.com/SrStamm/ai-toolkit.git
cd ai-toolkit
docker-compose up --build
```
