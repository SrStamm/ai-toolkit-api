# ai-toolkit

> **Versi√≥n actual:** `v2.2`  
> **Estado:** estable (educacional / experimental, con profiling real)

**Herramientas de IA para backend (FastAPI)**

`ai-toolkit` es una API educativa y experimental construida en FastAPI para explorar c√≥mo dise√±ar sistemas backend con LLMs de forma profesional, poniendo foco en:

- control expl√≠cito del comportamiento del modelo
- validaci√≥n estricta del output
- manejo consciente de errores y retries
- arquitectura desacoplada y extensible
- observabilidad y m√©tricas

> üéØ Objetivo del proyecto
> No es un producto final, sino un laboratorio backend para demostrar criterio arquitect√≥nico real en sistemas con IA: c√≥mo se dise√±an, c√≥mo evolucionan y c√≥mo se preparan para un entorno enterprise-like.

---

## Estado actual ‚Äì v2.2 (RAG asincr√≥nico + Observabilidad + Profiling real)

La versi√≥n v2.2 consolida el sistema como un backend RAG asincr√≥nico instrumentado profesionalmente, con:

- procesamiento desacoplado v√≠a Celery
- m√©tricas Prometheus completas
- dashboard Grafana operativo
- fallback entre LLM remoto y local
- circuit breaker funcional
- benchmarks comparativos reales
- profiling de throughput y latencia

### Objetivos alcanzados en v2.1 y v2.2

- Instrumentaci√≥n completa del pipeline RAG
- M√©tricas espec√≠ficas por etapa (vector search, LLM, Celery)
- Dashboard en Grafana con:
  - errores por etapa
  - fallbacks LLM
  - latencia promedio y percentiles
  - duraci√≥n de tasks Celery (P50, P95, P99)
- Comparaci√≥n emp√≠rica:
  - LLM remoto (Mistral)
  - LLM local (Ollama)
- Validaci√≥n real de circuit breaker
- Test de carga sobre ingesti√≥n (40+ documentos t√©cnicos)
- Inserci√≥n masiva en Qdrant (4096+ vectores generados v√≠a Sentence Transformers)
- Throughput controlado sin errores ni p√©rdida de tasks

### Arquitectura v2.2

```ascii
Cliente / Frontend
‚Üì
FastAPI (API layer)
  - Validaci√≥n
  - Creaci√≥n de job_id
  - Dispatch de tareas a Celery
‚Üì
Broker / Backend (Redis)
‚Üì
Celery Worker
  - Extracci√≥n
  - Limpieza
  - Chunking
  - Embeddings
  - Inserci√≥n en Vector Store
  - Actualizaci√≥n de estado en Redis
‚Üì
Respuesta a Frontend v√≠a job_id
```

## Benchmarks reales (V2.2)

Se realizaron pruebas controladas para medir:

### LLM remoto (Mistral)
- Latencia promedio: ~2‚Äì3s
- Sin errores
- Sin activaci√≥n de circuit breaker

### LLM local (Ollama)
- Latencia promedio: 20‚Äì40s
- CPU-bound
- Validaci√≥n de fallback autom√°tico

### Ingesti√≥n masiva (Celery)
- 40+ URLs t√©cnicas
- 4096 puntos vectoriales generados
- Duraci√≥n promedio de tasks: ~37‚Äì44s
- 0 errores
- Sistema estable bajo carga

### Observaciones t√©cnicas

- El sistema se comporta como CPU-bound durante generaci√≥n de embeddings (Sentence Transformers).
- El aumento de latencia bajo carga es consistente con saturaci√≥n controlada de CPU.
- No se detectaron:
  - deadlocks
  - p√©rdida de tasks
  - corrupci√≥n de vector store
  - memory leaks evidentes

---

## Validaci√≥n arquitect√≥nica

La arquitectura fue validada emp√≠ricamente mediante:

- tests de carga concurrentes
- profiling de latencia real
- comparaci√≥n de proveedores LLM
- observabilidad completa con Prometheus + Grafana
- separaci√≥n real entre API y procesamiento pesado

Este proyecto demuestra:

- dise√±o desacoplado
- tolerancia a fallos (fallback + circuit breaker)
- instrumentaci√≥n profesional
- capacidad de escalar horizontalmente (Celery workers)


---

## Funcionalidades del sistema

### Core RAG

- Ingesta de documentos v√≠a URL o archivos
- Chunking espec√≠fico por tipo de documento
- Strategy Pattern para chunking
- Embeddings locales y remotos con batching
- Re-ranking simple
- Construcci√≥n de contexto expl√≠cito para el LLM
- Streaming de respuesta
- Metadata por chunk (source, domain, topic, chunk_index)

### Observabilidad y m√©tricas

- Logs estructurados
- Decoradores de latencia por LLM y RAG
- M√©tricas Prometheus:
- Histogram de latencia por etapa
- Tokens consumidos
- Errores por etapa
- Fallbacks y circuit breaker
- M√©tricas espec√≠ficas para Celery:
- Duraci√≥n de tasks
- Status (success/error)
- Panel b√°sico de estado en Frontend

### Frontend

- Chat estilo RAG
- Inputs opcionales: dominio, topic
- Estado de carga y errores
- Citations por chunk
- Visualizaci√≥n parcial del progreso de tasks

---

## Roadmap de versiones siguientes

### V2.1 ‚Äì Observabilidad avanzada (completado)

- Histogram por etapa del RAG
- M√©tricas espec√≠ficas para Celery
- Dashboard Grafana operativo
- Percentiles P50, P95, P99

### V2.2 ‚Äì Performance profiling (completado)

- Benchmark LLM remoto vs local
- Validaci√≥n emp√≠rica de circuit breaker
- Medici√≥n de throughput Celery
- Inserci√≥n masiva en Qdrant
- An√°lisis CPU-bound vs I/O-bound

### V3.0 ‚Äì RAG avanzado y evaluaci√≥n (exploratorio)

> Objetivo: mejorar calidad

- Mejorar filtros sem√°nticos
- Mejor estrategia de metadata
- Hybrid search (BM25 + vector)

### V3.1

- Integrar RAGAS
- Medir faithfulness
- Medir answer relevancy
- Medir context precision

### V3.2

- Implementar versi√≥n con LlamaIndex
- Comparar:
  - Latencia
  - Recall
  - Calidad
  - Complejidad de c√≥digo

### V4.0

> Objetivo: Orquestaci√≥n

- Tool registry
- Skill abstraction
- Agente determin√≠stico (policy simple)

### V4.1

- Planner b√°sico
- Router entre:
  - RAG
  - Tool
  - Direct LLM


---

## Filosof√≠a de dise√±o

- Transparencia del flujo: cada paso del pipeline es trazable
- Separaci√≥n de responsabilidades: API, l√≥gica de negocio y proveedores desacoplados
- Control del riesgo: retries, errores y fallback expl√≠citos
- Intercambiabilidad de componentes: LLM, embeddings y vector stores reemplazables sin afectar el core

---

## Instalaci√≥n local r√°pida

```bash
git clone https://github.com/SrStamm/ai-toolkit.git
cd ai-toolkit
docker-compose up --build
```

