# ai-toolkit

> **Versi√≥n actual:** `v3.0`  
> **Estado:** estable (educacional / experimental, con RAG avanzado)

**Herramientas de IA para backend (FastAPI)**

`ai-toolkit` es una API educativa y experimental construida en FastAPI para explorar c√≥mo dise√±ar sistemas backend con LLMs de forma profesional, poniendo foco en:

- control expl√≠cito del comportamiento del modelo
- validaci√≥n estricta del output
- manejo consciente de errores y retries
- arquitectura desacoplada y extensible
- observabilidad y m√©tricas
- calidad del pipeline RAG

> üéØ Objetivo del proyecto
> No es un producto final, sino un laboratorio backend para demostrar criterio arquitect√≥nico real en sistemas con IA: c√≥mo se dise√±an, c√≥mo evolucionan y c√≥mo se preparan para un entorno enterprise-like.

---

## Estado actual ‚Äì v3.0 (RAG avanzado: Hybrid Search + Chunking sem√°ntico)

La versi√≥n v3.0 extiende la base s√≥lida de v2.2 hacia la mejora de **calidad del pipeline RAG**, introduciendo b√∫squeda h√≠brida real y una estrategia de chunking y metadata significativamente m√°s precisa.

### Qu√© cambi√≥ respecto a v2.2

**Hybrid Search (sparse + dense)**

La b√∫squeda vectorial ahora combina dos vectores por chunk:

- `dense`: embeddings sem√°nticos v√≠a Sentence Transformers
- `sparse`: vectores TF-IDF/BM25 para matching l√©xico exacto

La fusi√≥n se realiza con **RRF (Reciprocal Rank Fusion)** directamente en Qdrant, sin post-procesamiento manual. Esto mejora el recall en queries con t√©rminos t√©cnicos espec√≠ficos donde la b√∫squeda sem√°ntica sola falla.

**Chunking sem√°ntico por tipo de documento**

Cada tipo de documento tiene su propia estrategia de chunking, ahora con detecci√≥n de estructura y metadata enriquecida:

- `PDFCleaner`: limpieza profunda de artefactos (guiones rotos, l√≠neas de √≠ndice, TOC), detecci√≥n de headings por t√≠tulo case y numeraci√≥n, overlap consistente entre chunks
- `HTMLCleaner`: segmentaci√≥n por `h2`/`h3`, secci√≥n como anchor sem√°ntico
- `MarkdownCleaner`: split por `#`/`##`/`###`, heading preservado en texto y metadata

**Metadata enriquecida por chunk**

Todos los chunks ahora incluyen el campo `section`, que refleja el heading o secci√≥n del documento al que pertenece el chunk. Esto permite al LLM contextualizar mejor la respuesta y al reranker priorizar chunks con mayor relevancia estructural.

```json
{
  "text": "...",
  "section": "Hybrid Search and Retrieval",
  "source": "AI Engineering.pdf",
  "domain": "libros",
  "topic": "ia",
  "chunk_index": 142,
  "ingested_at": 1771870686
}
```

### Arquitectura v3.0

```ascii
Cliente / Frontend
‚Üì
FastAPI (API layer)
  - Validaci√≥n
  - Creaci√≥n de job_id
  - Dispatch de tareas a Celery
‚Üì
Broker / Backend (Redis)
‚Üì
Celery Worker
  - Extracci√≥n (URL / PDF / HTML / Markdown)
  - Limpieza espec√≠fica por tipo
  - Chunking sem√°ntico con detecci√≥n de secci√≥n
  - Embeddings h√≠bridos (dense + sparse)
  - Inserci√≥n en Vector Store con metadata enriquecida
  - Actualizaci√≥n de estado en Redis
‚Üì
Qdrant (Hybrid Search con RRF)
‚Üì
Reranker (Cross-Encoder)
‚Üì
LLM con contexto estructurado
‚Üì
Respuesta a Frontend v√≠a streaming
```

---

## Benchmarks reales (V2.2)

Se realizaron pruebas controladas para medir:

### LLM remoto (Mistral)

- Latencia promedio: ~2‚Äì3s
- Sin errores
- Sin activaci√≥n de circuit breaker

### LLM local (Ollama)

- Latencia promedio: 20‚Äì40s
- CPU-bound
- Validaci√≥n de fallback autom√°tico

### Ingesti√≥n masiva (Celery)

- 40+ URLs t√©cnicas
- 4096 puntos vectoriales generados
- Duraci√≥n promedio de tasks: ~37‚Äì44s
- 0 errores
- Sistema estable bajo carga

### Observaciones t√©cnicas

- El sistema se comporta como CPU-bound durante generaci√≥n de embeddings (Sentence Transformers).
- El aumento de latencia bajo carga es consistente con saturaci√≥n controlada de CPU.
- No se detectaron deadlocks, p√©rdida de tasks, corrupci√≥n de vector store ni memory leaks evidentes.

---

## Validaci√≥n arquitect√≥nica

La arquitectura fue validada emp√≠ricamente mediante:

- tests de carga concurrentes
- profiling de latencia real
- comparaci√≥n de proveedores LLM
- observabilidad completa con Prometheus + Grafana
- separaci√≥n real entre API y procesamiento pesado

Este proyecto demuestra:

- dise√±o desacoplado
- tolerancia a fallos (fallback + circuit breaker)
- instrumentaci√≥n profesional
- capacidad de escalar horizontalmente (Celery workers)
- mejora iterativa de calidad RAG sin romper infraestructura

---

## Funcionalidades del sistema

### Core RAG

- Ingesta de documentos v√≠a URL o archivos (PDF, HTML, Markdown)
- Chunking sem√°ntico espec√≠fico por tipo de documento
- Detecci√≥n de secci√≥n/heading por documento
- Strategy Pattern para chunking
- Embeddings h√≠bridos (dense + sparse) con batching
- Hybrid Search con RRF en Qdrant
- Re-ranking con Cross-Encoder
- Metadata enriquecida por chunk (source, section, domain, topic, chunk_index)
- Construcci√≥n de contexto expl√≠cito para el LLM
- Streaming de respuesta

### Observabilidad y m√©tricas

- Logs estructurados
- Decoradores de latencia por LLM y RAG
- M√©tricas Prometheus: histogram de latencia por etapa, tokens consumidos, errores, fallbacks y circuit breaker
- M√©tricas espec√≠ficas para Celery: duraci√≥n de tasks, status (success/error)
- Panel b√°sico de estado en Frontend

### Frontend

- Chat estilo RAG
- Inputs opcionales: dominio, topic
- Estado de carga y errores
- Citations por chunk
- Visualizaci√≥n parcial del progreso de tasks

---

## Roadmap de versiones

### V2.1 ‚Äì Observabilidad avanzada (completado)

- Histogram por etapa del RAG
- M√©tricas espec√≠ficas para Celery
- Dashboard Grafana operativo
- Percentiles P50, P95, P99

### V2.2 ‚Äì Performance profiling (completado)

- Benchmark LLM remoto vs local
- Validaci√≥n emp√≠rica de circuit breaker
- Medici√≥n de throughput Celery
- Inserci√≥n masiva en Qdrant
- An√°lisis CPU-bound vs I/O-bound

### V3.0 ‚Äì RAG avanzado: calidad (completado)

- Hybrid Search (dense + sparse) con RRF
- Chunking sem√°ntico por tipo de documento
- Detecci√≥n de secci√≥n/heading como metadata
- Limpieza profunda de PDFs (TOC, artefactos, headings)
- `ChunkWithMetadata` como contrato entre cleaner y vector store

### V3.1 ‚Äì Evaluaci√≥n RAG (pr√≥ximo)

- Integrar RAGAS
- Medir faithfulness
- Medir answer relevancy
- Medir context precision

### V3.2 ‚Äì LlamaIndex (exploratorio)

- Implementar versi√≥n equivalente con LlamaIndex
- Comparar latencia, recall, calidad y complejidad de c√≥digo

### V4.0 ‚Äì Agente (futuro)

- Tool registry
- Skill abstraction
- Agente determin√≠stico (policy simple)
- Planner b√°sico con router entre RAG, Tool y Direct LLM

---

## Filosof√≠a de dise√±o

- Transparencia del flujo: cada paso del pipeline es trazable
- Separaci√≥n de responsabilidades: API, l√≥gica de negocio y proveedores desacoplados
- Control del riesgo: retries, errores y fallback expl√≠citos
- Intercambiabilidad de componentes: LLM, embeddings y vector stores reemplazables sin afectar el core
- Mejora iterativa: cada versi√≥n mejora una dimensi√≥n distinta (infraestructura ‚Üí observabilidad ‚Üí calidad)

---

## Instalaci√≥n local r√°pida

```bash
git clone https://github.com/SrStamm/ai-toolkit.git
cd ai-toolkit
docker-compose up --build
```
